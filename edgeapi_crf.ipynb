{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  CRF + expert.ai edge NL API for named entities recognition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "CoNLL corpus is download and prepared for the training phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods for processing CoNLL corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONLL_URL_ROOT = \"https://raw.githubusercontent.com/nluninja/nlp_datasets/be9fd23409f1443790f6e1eab91d28b105769368/conll2003/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import re\n",
    "import urllib\n",
    "import pandas as pd\n",
    "from math import nan\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import metrics\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_conll_data(filename, url_root=CONLL_URL_ROOT, \n",
    "                    only_tokens=False):\n",
    "    \"\"\"\n",
    "    Take an url to the raw .txt files that you can find the repo linked above,\n",
    "    load data and save it into a list of tuples data structure.\n",
    "    \n",
    "    Those files structure data with a word in each line with word, POS, \n",
    "    syntactic tag and entity tag separated by a whitespace. Sentences are \n",
    "    separated by an empty line.\n",
    "    \"\"\"\n",
    "    lines = read_raw_conll(url_root, filename)\n",
    "    X = []\n",
    "    Y = []\n",
    "    sentence = []\n",
    "    labels = []\n",
    "    output_labels=set()\n",
    "    for line in lines:\n",
    "        if line == \"\\n\":\n",
    "            if(len(sentence) != len(labels)):\n",
    "                print(f\"Error: we have {len(sentence)} words but {len(labels)} labels\")\n",
    "            if sentence and is_real_sentence(only_tokens, sentence):\n",
    "                X.append(sentence)\n",
    "                Y.append(labels)\n",
    "            sentence = []\n",
    "            labels = []\n",
    "        else:\n",
    "            features = line.split()\n",
    "            tag = features.pop()\n",
    "            labels.append(tag)\n",
    "            output_labels.add(tag)\n",
    "            if only_tokens:\n",
    "                sentence.append(features.pop(0))\n",
    "            else:\n",
    "                sentence.append(tuple(features))\n",
    "    \n",
    "    print(f\"Read {len(X)} sentences\")\n",
    "    if(len(X) != len(Y)):\n",
    "        print(\"ERROR in reading data.\")\n",
    "    return X, Y, output_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_raw_conll(url_root, filename):\n",
    "    \"\"\"Read a file which contains a conll03 dataset\"\"\"\n",
    "    lines = []\n",
    "    full_url = url_root + filename\n",
    "    lines = open_read_from_url(full_url)\n",
    "    return lines[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_read_from_url(url):\n",
    "    \"\"\"\n",
    "    Take in input an url to a .txt file and return the list of its raws\n",
    "    \"\"\"\n",
    "    print(f\"Read file from {url}\")\n",
    "    file = urllib.request.urlopen(url)\n",
    "    lines = []\n",
    "    for line in file:\n",
    "        lines.append(line.decode(\"utf-8\"))\n",
    "\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_real_sentence(only_token, sentence):\n",
    "    \"\"\"Chek if a sentence is a real sentence or a document separator\"\"\"\n",
    "    first_word = \"\"\n",
    "    if only_token:\n",
    "        first_word = sentence[0]\n",
    "    else:\n",
    "        first_word = sentence[0][0]\n",
    "\n",
    "    if '---------------------' in first_word or first_word == '-DOCSTART-':\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read file from https://raw.githubusercontent.com/nluninja/nlp_datasets/be9fd23409f1443790f6e1eab91d28b105769368/conll2003/data/train.txt\n",
      "Read 14028 sentences\n",
      "Read file from https://raw.githubusercontent.com/nluninja/nlp_datasets/be9fd23409f1443790f6e1eab91d28b105769368/conll2003/data/valid.txt\n",
      "Read 3250 sentences\n",
      "Read file from https://raw.githubusercontent.com/nluninja/nlp_datasets/be9fd23409f1443790f6e1eab91d28b105769368/conll2003/data/test.txt\n",
      "Read 3453 sentences\n"
     ]
    }
   ],
   "source": [
    "raw_train, y_train, output_labels = load_conll_data('train.txt', only_tokens=True)\n",
    "raw_valid, y_valid, _ = load_conll_data('valid.txt', only_tokens=True)\n",
    "raw_test, y_test, _ = load_conll_data('test.txt', only_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']\n",
      "['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "print(raw_train[0])\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature generation with edge NL API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"EAI_USERNAME\"] = 'andrea.belli@gmail.com'\n",
    "os.environ[\"EAI_PASSWORD\"] = 'eXpert00!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from expertai.nlapi.edge.client import ExpertAiClient\n",
    "client = ExpertAiClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods for performing tokenization and features generation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_docs(raw, eai):\n",
    "    \"\"\"Analyze a sentence with expertai\n",
    "    \n",
    "    Take a list of sentences, where each sentence is a list of token; build a\n",
    "    string with the sentence and analyze it with expertai.\n",
    "    \n",
    "    Params:\n",
    "        raw: list of lists of tokens\n",
    "        eai: Expertai instance\n",
    "    Return:\n",
    "        docs: list of expertai Document\n",
    "    \"\"\"\n",
    "    docs = []\n",
    "    for sent in tqdm(raw):\n",
    "        docs.append(eai.full_analysis(' '.join(sent)))\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_label(doc, syncon):\n",
    "    \"\"\"Extract the knowledge label of a syncon in a document, if any\"\"\"\n",
    "    label = ''\n",
    "    if hasattr(doc, 'knowledge'):\n",
    "        for element in doc.knowledge:\n",
    "            if element.syncon == syncon:\n",
    "                label = element.label\n",
    "                break\n",
    "        if label and '.' in label:\n",
    "            label = label.split('.')[-1]\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_from_docs (sentences, docs):\n",
    "    \"\"\"Extract token features from expertai docs\n",
    "    \n",
    "    Given a list of tokenized sentences and the relative expertai docs, \n",
    "    create a dictionary for each with the doc features:\n",
    "        * Word\n",
    "        * PoS tag;\n",
    "        * Dep tag;\n",
    "        * Syncon;\n",
    "        * Label;\n",
    "        * Typeclass (a mix of POS and entity);\n",
    "    Params:\n",
    "        sentences: list of sentences, that are lists of strings;\n",
    "        docs: list of expertai Document;\n",
    "    Returns:\n",
    "        eai_sents: list of sentences features, that are lists of dictionaries;\n",
    "    \"\"\"\n",
    "    eai_sents = []\n",
    "    for sent_idx in trange(len(sentences)):\n",
    "        seek = 0    # Index of the part of the sentence string already read\n",
    "        eai_tokenlist = []\n",
    "        for tk_idx in range(len(sentences[sent_idx])):\n",
    "            # Token text and boundary indexes in doc.content\n",
    "            token = sentences[sent_idx][tk_idx]\n",
    "            index_start = docs[sent_idx].content.find(token, seek)\n",
    "            index_end = index_start + len(token)\n",
    "            possible_tokens = []\n",
    "            for t in docs[sent_idx].tokens:\n",
    "                # If a eai Token contain (part of the) chunk od text, it can be\n",
    "                # the possible corresponding Token\n",
    "                if (t.start<=index_start and t.end>=index_end) or \\\n",
    "                (t.start >= index_start and t.start <= index_end) or \\\n",
    "                (t.end >= index_start and t.end <= index_end):\n",
    "                    possible_tokens.append(t)\n",
    "            if not possible_tokens:\n",
    "                print('ERROR: expertai tokenization not found for token', token)\n",
    "                eai_tokenlist.append(_voidtoken())\n",
    "            else:\n",
    "                # Extract information from the eai.Token for the raw token we \n",
    "                # are analyzing\n",
    "                if len(possible_tokens)>1:\n",
    "                    possible_tokens.sort(key = lambda t: t.syncon, reverse=True)\n",
    "                new_token = {\n",
    "                    'word': token,\n",
    "                    'pos': possible_tokens[0].pos,\n",
    "                    'syncon': possible_tokens[0].syncon,\n",
    "                    'ancestor': -1,\n",
    "                    'label': _get_label(docs[sent_idx], possible_tokens[0].syncon),\n",
    "                    'dep': possible_tokens[0].dependency.label,\n",
    "                    'typeclass': possible_tokens[0].type_.split('.')\n",
    "                }\n",
    "                eai_tokenlist.append(new_token)\n",
    "            seek = index_end\n",
    "            while len(docs[sent_idx].content) < seek and (docs[sent_idx].content[seek] == ' '):\n",
    "                seek += 1\n",
    "        eai_sents.append(eai_tokenlist)\n",
    "    return eai_sents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_from_word(sentence, idx):\n",
    "    \"\"\"Extract features related to a word and its neighbours\"\"\"\n",
    "    token = sentence[idx] \n",
    "    \n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': token['word'].lower(),\n",
    "        'word[-3:]': token['word'][-3:],\n",
    "        'word[-2:]': token['word'][-2:],\n",
    "        'word.isupper()': token['word'].isupper(),\n",
    "        'word.istitle()': token['word'].istitle(),\n",
    "        'word.isdigit()': token['word'].isdigit(),\n",
    "        'eai.postag': token['pos'],\n",
    "        'eai.postag[:2]': token['pos'][:2],\n",
    "        'eai.deptag': token['dep'],\n",
    "        'eai.deptag[-2:]': token['dep'][-2:],\n",
    "        'eai.syncon': -1 if token['syncon'] == -1 else token['syncon'] / 10000.,\n",
    "        'eai.ancestor': -1 if token['ancestor'] == -1 else token['ancestor'] / 10000.,\n",
    "        'eai.labels': token['label'],\n",
    "        'eai.typeclass': token['typeclass'],\n",
    "    }\n",
    "    if idx > 0:\n",
    "        token1 = sentence[idx-1]\n",
    "        features.update({\n",
    "            '-1:word.lower()': token1['word'].lower(),\n",
    "            '-1:word.istitle()': token1['word'].istitle(),\n",
    "            '-1:word.isupper()': token1['word'].isupper(),\n",
    "            '-1:eai.postag': token1['pos'],\n",
    "            '-1:eai.deptag': token1['dep'],\n",
    "            '-1:eai.labels': token1['label'],\n",
    "            '-1:eai.typeclass': token1['typeclass'],\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "        \n",
    "    if idx < len(sentence)-1:\n",
    "        token1 = sentence[idx-1]\n",
    "        features.update({\n",
    "            '+1:word.lower()': token1['word'].lower(),\n",
    "            '+1:word.istitle()': token1['word'].istitle(),\n",
    "            '+1:word.isupper()': token1['word'].isupper(),\n",
    "            '+1:eai.postag': token1['pos'],\n",
    "            '+1:eai.deptag': token1['dep'],\n",
    "            '+1:eai.labels': token1['label'],\n",
    "            '+1:eai.typeclass': token1['typeclass'],\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "                \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_from_sentence(sentence):\n",
    "    \"\"\"Create feature dictionary for a sentence\"\"\"\n",
    "    return tuple(features_from_word(sentence, index) for index in range(len(sentence)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _voidtoken():\n",
    "    \"\"\"Generate an empty token\"\"\"\n",
    "    t = {\n",
    "        'word': '',\n",
    "        'pos': '',\n",
    "        'syncon': -1,\n",
    "        'ancestor': -1,\n",
    "        'dep': '',\n",
    "        'label': ''\n",
    "    }\n",
    "    return t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate tokens and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|█████████████████████████████▉                                            | 5673/14028 [19:29<25:19,  5.50it/s]"
     ]
    }
   ],
   "source": [
    "train_docs = tokens_to_docs(raw_train, client)\n",
    "test_docs = tokens_to_docs(raw_test, client)\n",
    "valid_docs = tokens_to_docs(raw_valid, client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = features_from_docs(raw_train, train_docs)\n",
    "test = features_from_docs(raw_test, test_docs)\n",
    "valid = features_from_docs(raw_valid, valid_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "p_idx=2\n",
    "print(raw_train[p_idx])\n",
    "print(y_train[p_idx])\n",
    "print('')\n",
    "pprint.pprint(train[p_idx])\n",
    "print('')\n",
    "pprint.pprint([tk.__dict__ for tk in train_docs[p_idx].tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [features_from_sentence(sentence) for sentence in train]\n",
    "X_test = [features_from_sentence(sentence) for sentence in test]\n",
    "X_valid = [features_from_sentence(sentence) for sentence in valid]\n",
    "pprint.pprint(X_train[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "crf = None\n",
    "gs = None\n",
    "\n",
    "\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm = 'lbfgs',\n",
    "    c1 = 0.1,\n",
    "    c2 = 0.5,\n",
    "    max_iterations = 800,\n",
    "    all_possible_transitions = True,\n",
    "    verbose = True\n",
    "    )\n",
    "crf.fit(X_train, y_train, X_dev=X_valid, y_dev=y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def compute_prediction_latency(dataset, model, n_instances=-1):\n",
    "    \"\"\"Compute prediction latency of a model.\n",
    "    \n",
    "    The model must have a predict method.\n",
    "    \"\"\"\n",
    "    if n_instances == -1:\n",
    "        n_instances = len(dataset)\n",
    "    start_time = time.process_time()\n",
    "    model.predict(dataset)\n",
    "    total_latency = time.process_time() - start_time\n",
    "    return total_latency / n_instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model size: {:0.2f}M'.format(crf.size_ / 1000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Model latency in prediction: {compute_prediction_latency(X_test, crf):.3} s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [ ('Test Set', X_test, y_test), ('Validation Set', X_valid, y_valid)]\n",
    "\n",
    "for title, X, Y in datasets:\n",
    "    Y_pred = crf.predict(X)\n",
    "    print(title)\n",
    "    print(classification_report(Y, Y_pred, digits=3))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
